{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/aisummer/ssemenov_workspace/nlp-service\")\n",
    "\n",
    "import pandas as pd \n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "from src.DocumentsParser.utils import DBS_DIR_DENSE_VECTORDB_NAME, DBS_DIR_SPARSE_VECTORDB_NAME\n",
    "from src.evaluation_metrics import ReaderMetrics\n",
    "from src.DocumentsSummarizer.Summarizer import SummarizerModule\n",
    "from src.LLM_build.utils import llmConfig\n",
    "from src.utils import DialogueState\n",
    "from src.logger import Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Создаём объекты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = SummarizerModule()\n",
    "config = DialogueState()\n",
    "configLLM = llmConfig()\n",
    "\n",
    "options = {\n",
    "\"num_keep\": configLLM.num_keep,\n",
    "\"seed\": configLLM.seed,\n",
    "\"num_predict\": configLLM.num_predict,\n",
    "\"top_k\": configLLM.top_k,\n",
    "\"top_p\": configLLM.top_p,\n",
    "\"tfs_z\": configLLM.tfs_z,\n",
    "\"typical_p\": configLLM.typical_p,\n",
    "\"repeat_last_n\": configLLM.repeat_last_n,\n",
    "\"temperature\": configLLM.temperature,\n",
    "\"repeat_penalty\": configLLM.repeat_penalty,\n",
    "\"presence_penalty\": configLLM.presence_penalty,\n",
    "\"frequency_penalty\": configLLM.frequency_penalty,\n",
    "\"mirostat\": configLLM.mirostat,\n",
    "\"mirostat_tau\": configLLM.mirostat_tau,\n",
    "\"mirostat_eta\": configLLM.mirostat_eta,\n",
    "\"penalize_newline\": configLLM.penalize_newline,\n",
    "#\"stop\": config.stop,\n",
    "\"numa\": configLLM.numa,\n",
    "\"num_ctx\": configLLM.num_ctx,\n",
    "\"num_batch\": configLLM.num_batch,\n",
    "\"num_gpu\": configLLM.num_gpu,\n",
    "\"main_gpu\": configLLM.main_gpu,\n",
    "\"low_vram\": configLLM.low_vram,\n",
    "\"f16_kv\": configLLM.f16_kv,\n",
    "\"vocab_only\": configLLM.vocab_only,\n",
    "\"use_mmap\": configLLM.use_mmap,\n",
    "\"use_mlock\": configLLM.use_mlock,\n",
    "\"num_thread\": configLLM.num_thread\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Указываем инфо для данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARKS_INFO = {'sberquad': 'v1', 'squadv2': 'v1'}\n",
    "SAVE_LOGFILE = '/home/aisummer/ssemenov_workspace/nlp-service/benchmark/logs/trial1.json'\n",
    "banchmark_paths = {}\n",
    "for name, version in BENCHMARKS_INFO.items():\n",
    "    banchmark_paths[name] = {\n",
    "        'table': f\"../data/{name}/tables/{version}/benchmark.csv\",\n",
    "        'dense_db': f\"../data/{name}/dbs/{version}/{DBS_DIR_DENSE_VECTORDB_NAME}\",\n",
    "        'sparse_db':  f\"../data/{name}/dbs/{version}/{DBS_DIR_SPARSE_VECTORDB_NAME}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Meteor...\n",
      "Loading ExactMatch\n"
     ]
    }
   ],
   "source": [
    "# загрузить benchmark-датасет\n",
    "benchmarks_df = {}\n",
    "for name, bench_path in banchmark_paths.items():\n",
    "    benchmarks_df[name] = pd.read_csv(banchmark_paths[name]['table'], sep=';')\n",
    "\n",
    "# инициализировать класс с метриками\n",
    "base_dir = '/home/aisummer/ssemenov_workspace/nlp-service'\n",
    "metrics = ReaderMetrics(base_dir)\n",
    "\n",
    "# logging\n",
    "logger = Logger(False)\n",
    "log = logger.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.']\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks_df['squadv2']['contexts'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_ans(df: dict, reader: SummarizerModule) -> list:\n",
    "    relevant_ans = []\n",
    "    question = df['question']\n",
    "    contexts = df['contexts']\n",
    "\n",
    "    for i in range(20):\n",
    "        reader.create_answer(config, configLLM, question[i], contexts[i].strip('[]').strip(\"'\"))\n",
    "        relevant_ans.append(config.answer)\n",
    "    return relevant_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rouge:  0.2\n",
      "bleu:  0.05811\n",
      "meteor:  0.44569\n",
      "exact_match:  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "benchmarks_score = []\n",
    "\n",
    "predicted_sberquad = get_relevant_ans(benchmarks_df['sberquad'], reader)\n",
    "targets_sberquad = benchmarks_df['sberquad']['answer'].to_list()\n",
    "\n",
    "# predicted_squadv2 = get_relevant_ans(benchmarks_df['squadv2'], reader)\n",
    "# targets_squadv2 = benchmarks_df['squadv2']['answer'].to_list()\n",
    "\n",
    "def metric(benchmarks_score:list[dict], predicted: list[str], targets: list[str], name:str) -> None:\n",
    "    \n",
    "    length = len(predicted)\n",
    "    # print('Question: ', benchmarks_df['sberquad']['question'][i])\n",
    "    # print('Context: ', benchmarks_df['sberquad']['contexts'][i].strip('[]').strip(\"'\"))\n",
    "    # print('Predicted: ', predicted[i])\n",
    "    # print('Target: ', targets[i])\n",
    "\n",
    "    rouge_score = metrics.rouge(predicted, targets[:length])\n",
    "    print(\"rouge: \", rouge_score)\n",
    "\n",
    "    bleu_score = metrics.bleu(predicted, targets[:length])\n",
    "    print(\"bleu: \", bleu_score)\n",
    "\n",
    "    meteor = metrics.meteor(predicted, targets[:length])\n",
    "    print(\"meteor: \", meteor)\n",
    "\n",
    "    exact_match = metrics.exact_match(predicted, targets[:length])\n",
    "    print(\"exact_match: \", exact_match)\n",
    "\n",
    "    # посчитать метрики \n",
    "    score = {\n",
    "        'name': name,\n",
    "        'rouge': rouge_score,\n",
    "        'bleu': bleu_score,\n",
    "        'meteor': meteor,\n",
    "        'exact_match': exact_match\n",
    "    }\n",
    "\n",
    "    benchmarks_score.append(score)\n",
    "\n",
    "\n",
    "\n",
    "metric(benchmarks_score, predicted_sberquad, targets_sberquad,'sberquad')\n",
    "# metric(benchmarks_score, predicted_squadv2, targets_squadv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sberquad',\n",
       "  'rouge': 0.2,\n",
       "  'bleu': 0.05811,\n",
       "  'meteor': 0.44569,\n",
       "  'exact_match': 0.0}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './logs/trial1.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m      6\u001b[0m log_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m: BENCHMARKS_INFO, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: options\u001b[38;5;241m.\u001b[39mitems}\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSAVE_LOGFILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m      9\u001b[0m    fd\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(log_data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.nlp_env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './logs/trial1.json'"
     ]
    }
   ],
   "source": [
    " # сохранить результат\n",
    "if os.path.exists(SAVE_LOGFILE):\n",
    "    print(\"Файл существует!\")\n",
    "    raise ValueError\n",
    "\n",
    "log_data = {'info': BENCHMARKS_INFO, 'params': options.items}\n",
    "\n",
    "with open(SAVE_LOGFILE, 'w', encoding='utf-8') as fd:\n",
    "    fd.write(json.dumps(log_data, indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
