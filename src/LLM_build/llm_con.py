from ollama import Client

# Создаём объект для обращения к модели
client = Client(host='http://172.20.6.160:11434')

def chat_llm(promt:str) -> str:
    # Задаём system, assistant и user промпты
    response = client.chat(

        model='llama3', 
        messages=[
        {"role": "system", "content": "Ты вопросно-ответная система. Все ответы генерируешь на русском языке. Отвечай коротко по вопросам."},
        {"role": "assistant", "content": "Твоя база знаний, все ответы генерируются на основе текста ниже: \
        4.1.2 Вагон должен соответствовать климатическому исполнению УХЛ1 по ГОСТ 15150 с  \
        обеспе-чением работоспособного состояния в диапазоне рабочих температур от минус 60 °С до плюс 50 °С. 4.1.3 \
        Габарит вагона должен соответствовать требованиям ГОСТ 9238. 4.1.4 Вагон должен иметь кузов, включающий в себя \
        раму с установленными на ней отдельными емкостями (бункерами) для размещения грузов, а также иные устройства, \
        предусмотренные конструк-торской документацией, и должен быть оборудован: а) тележками по ГОСТ 9246 или иному стандарту, \
            распространяющемуся на тележки грузовых вагонов железных дорог; 6) автосцепными устройствами по ГОСТ 33434 или иному стандарту, \
        распространяющемуся на автосцепные устройства грузовых вагонов, с контуром зацепления автосцепки по ГОСТ 21447, \
        с обо-рудованием автосцепок нижним ограничителем вертикальных перемещений и расцепным приводом с \
        блокировочной цепью и поглощающими аппаратами по ГОСТ 32913; в) автоматическим пневматическим тормозом по ГОСТ 34434; \
        г) стояночным тормозом по ГОСТ 32880. "},
        {"role": "user", "content": promt + '\n'}],
        stream=True,
        temperature=0.8,
        max_tokens=300)

    # Выводим ответ модели
    for chunk in response:
        print(chunk['message']['content'], end='', flush=True)


if __name__ == "__main__":
    question = 'Требованиями какого госта должен соответствовать габарит вагона?'
    chat_llm(question)